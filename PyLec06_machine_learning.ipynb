{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    m = np.max(x)\n",
    "    ex = np.exp(x-m)\n",
    "    return ex / np.sum(ex)\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기계학습(machine learning)의 기본개념\n",
    "\n",
    "## 데이터의 종류\n",
    "\n",
    "기계학습은 데이터를 예측하는 모델의 정확한 변수값을 모르는 상태에서 이미 주어진 데이터를 통해 최적의 변수를 찾아내는 과정입니다.\n",
    "\n",
    "이를 위해 훈련 데이터(training data)와 시험 데이터(test data)가 필요합니다.\n",
    "\n",
    "> 훈련 데이터는 기본적으로 변수값을 최적화시키기 위해 사용되는 데이터 입니다.\n",
    ">\n",
    "> 훈련 데이터가 부정확하다면, 결과도 부정확합니다. (**Garbage in, garbage out**)\n",
    ">\n",
    "> 따라서 기계학습을 성공적으로 수행하기 위한 첫번째 단계는 좋은 훈련 데이터를 준비하는 것입니다.\n",
    "\n",
    "훈련 데이터를 이용하여 변수값을 찾았다면 이것이 정확한지 테스트 하기 위해 사용하는 데이터가 시험 데이터입니다.\n",
    "\n",
    "일반적으로 시험 데이터는 훈련 데이터의 크기보다 작습니다.\n",
    "\n",
    "보통 데이터는 훈련용과 시험용이 구분되어 있지 않으므로 무작위로 데이터를 구분하여 사용합니다.\n",
    "\n",
    "이를 미니배치(mini-batch)라 합니다.\n",
    "\n",
    "> 만약 시험 데이터가 지나치게 적거나 편향되어 있다면 어떤 문제가 생길 수 있나요?\n",
    ">\n",
    "> 모델이 훈련 데이터에 지나치게 최적화 된 상태를 오버피팅(overfitting)이라고 합니다.\n",
    ">\n",
    "> 오버피팅은 잘못된 모델링으로 인해 일어날 수도 있지만, 잘못된 데이터로 인해 일어날 수도 있습니다.\n",
    ">\n",
    "> 예를 들어 사진을 '개' 또는 '고양이'를 분류하는 모델을 학습시킨다고 합시다.\n",
    ">\n",
    "> '개'의 사진만을 가지고 학습 시킨 모델은 '개'는 정확히 맞출 수 있으나 '고양이'는 정확히 맞출 수가 없습니다.\n",
    ">\n",
    "> (오히려 '고양이'를 '개'라고 잘못 분류하는 경우가 더 많을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수(loss function)\n",
    "\n",
    "모델링을 최적화시키기 위해서는 정확도가 향상되는 방향으로 조금씩 변수값을 조정해야 합니다.\n",
    "\n",
    "다시 말하면 정확도가 떨어짐으로서 발생하는 손실(loss)이 최소화되어야 합니다.\n",
    "\n",
    "손실 함수란 기계 학습의 정확도를 가리키는 지표입니다.\n",
    "\n",
    "### 1. 평균 제곱 오차(mean squre error)\n",
    "\n",
    "평균제곱오차는 가장 많이 쓰이는 손실함수의 하나입니다.\n",
    "\n",
    "모델 예측값 $y_k$라 하고 실제값을 $t_k$라 하면 평균 제곱 오차 $E$는 다음과 같습니다.\n",
    "\n",
    "$$E = \\frac{1}{2}\\sum_{k=1}^N(y_k-t_k)^2$$\n",
    "\n",
    "여기서 $N$은 데이터의 크기입니다. \n",
    "\n",
    "편차의 제곱을 하는 이유는 참값($t_k$)보다 실제값($y_k$)이 작거나 큰 경우를 동일하게 보기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msq(y,t):\n",
    "    return .5 * np.sum((y-t) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 교차 엔트로피 오차\n",
    "\n",
    "교차 엔트로피 오차는(cross-entropy error)는 평균 제곱 오차와는 약간 성격이 다릅니다. 모델 예측값을 $y_k$ 참값을 $t_k$라 합시다. 교차 엔트로피 오차는 다음과 같이 정의됩니다.\n",
    "\n",
    "$$E = -\\sum_k t_k\\log y_k$$\n",
    "\n",
    "만약 예측하고자 하는 값이 참(1) 또는 거짓(0)이라면 모든 $t_k$는 $0$ 또는 $1$로 기록되어 있습니다. 그러나 예측값 $y_k$는 $0$에서 $1$사이의 실수값으로 기록될 것입니다. 먄약 $t_k$가 $0$이라면 $t_k\\log y_k$는 항상 $0$입니다. 만약 $t_k$가 $1$이라면 $t_k\\log y_k$는 $y_k$이 클수록 더 작은 음의 실수를 가리킵니다. 오차값 $E$에 음의 부호가 붙어있으므로, $E$는 항상 양수입니다. 또한 $t_k$가 $1$인 $y_k$가 $1$에 가까울수록 $E$가 작아짐을 알 수 있습니다. 다시 말해, 참값이 $1$인 데이터를 잘 예측할 수록 $E$가 작아집니다.\n",
    "\n",
    "실제로, 예측값 $y_k$가 $0$이 되면 $\\log$값이 마이너스 무한대를 반환하므로 계산에 오류가 발생합니다. 따라서 실제 구현에서는 $y_k$ 값에 아주 작은 $\\delta$를 더하여 $0$이 되지 않게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 교차 엔트로피에 사용된 레이블 $y_k$, $t_k$는 하나의 데이터에 대한 출력 뉴런의 값과 정답값입니다. 예를 들어 MNIST 데이터셋의 경우, 하나의 데이터마다 10개의 레이블(0,1,2,...,9)이 있습니다. 기계학습을 위해서는 아주 많은 수의 데이터를 학습시켜야 하고, 이것에 대한 교차 엔트로피를 구해야 합니다. 즉, $N$개의 데이터가 있다고 하면 각 $n$번째 데이터마다 교차엔트로피를 구해서 평균을 구합니다.\n",
    "$$E = -\\frac{1}{N}\\sum_n\\sum_k t_{nk}\\log y_{nk}$$\n",
    "그런데 이렇게 하면 계산에 많은 시간이 소요됩니다. MNIST 데이터만 해도 6만개의 데이터가 있습니다. 따라서 모든 데이터를 사용하지 않고, 랜덤으로 몇개의 샘플만 추출해서 계산하는 것이 효율적입니다. 이를 **미니배치(mini-batch)**라 합니다. `np.random.choice`함수를 이용하면 무작위로 원하는 갯수의 샘플을 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_minibatch(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기울기벡터(gradient)\n",
    "\n",
    "위와 같은 손실함수의 값을 줄이기 위해서는 현재 가중치와 편향을 손실함수가 작아지는 쪽으로 변화시켜야가야 합니다. 이를 위해 각 기울기벡터(gradient)를 구해야 합니다. 함수 $f(x,y)$에 대한 기울기 벡터는 다음과 같습니다.\n",
    "$$\\nabla f(x_0,y_0) = (f_x(x_0,y_0),f_y(x_0,y_0))$$\n",
    "이를 함수로 구현하면 아래와 같습니다. 실제로 편미분을 구현하기는 복잡하므로, 여기서는 수치적 편미분을 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_1d(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_2d(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    for row in range(x.shape[0]):\n",
    "        for col in range(x.shape[1]):\n",
    "            tmp_val = x[row, col]\n",
    "            x[row, col] = tmp_val + h\n",
    "            fxh1 = f(x)\n",
    "        \n",
    "            x[row, col] = tmp_val - h\n",
    "            fxh2 = f(x)\n",
    "        \n",
    "            grad[row, col] = (fxh1 - fxh2) / (2 * h)\n",
    "            x[row, col] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사하강법(Gradient descent)\n",
    "경사하강법이란 특점 점에서 기울기벡터 방향으로 조금씩 이동하면서 함수의 극소값을 도달하는 방법입니다. 다시말해 $x^{(0)} = (x_0^{(0)},x_1^{(0)})$에서 시작하였을 때, 음의 기울기 벡터 $-\\nabla f(x_0,x_1)$의 방향으로 $\\nu$만큼 이동하여 극소값에 근접한 새로운 점 $x^{(1)} = (x_0^{(1)}, x_1^{(1)}$을 찾는 방법입니다. 이를 수식으로 표현하면 아래와 같습니다.\n",
    "$$x_0^{(1)} = x_0^{(0)} - \\nu\\frac{\\partial f}{\\partial x_0}$$\n",
    "$$x_1^{(1)} = x_1^{(0)} - \\nu\\frac{\\partial f}{\\partial x_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 간단한 신경망 모델\n",
    "경사하강법을 이용하여 기계 학습이 구현된 간단한 신경망 모델을 만들어 보겠습니다. 여기서는 **클래스(class)**라는 구조를 사용합니다. 클래스란 딕셔너리 자료형과 비슷하게 변수가 구현될 뿐만 아니라 메서드 함수도 구현되는 자료구조를 말합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    m = np.max(x)\n",
    "    ex = np.exp(x - m)\n",
    "    return ex / np.sum(ex)\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_minibatch(y, t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 `simpleNet`이라는 클래스는 앞서 정의한 `network`를 클래스로 구현한 것입니다. `__init__`은 클래스객체를 초기화하는 함수로서 모든 클래스가 기본으로 구현되어야 합니다. 여기서는 랜덤한 2행 3열의 행렬로 초기화를 시킵니다. 예를 들어 `net`으로 클래스를 생성해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.11724359 -0.69089631  0.43287905]\n",
      " [-0.14126068  0.78843414 -0.50883815]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 입력 뉴런 값을 임의로 집어넣고 순전파(forward propagation)를 진행해 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54321154  0.29505294 -0.1982269 ]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([.6, .9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 실제값에 대한 참값 레이블이 $[0, 1, 0]$ 이었다고 합니다. (즉, 2번째 뉴런이 가장 크게 활성화가 되어야 정확한 결과입니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35401535236446646"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 1, 0])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 손실함수의 값을 더 줄이기 위해 경사하강법을 손실함수 $f$에 적용해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08862636 -0.13085045  0.04222409]\n",
      " [ 0.13293954 -0.19627567  0.06333613]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 기울기벡터 방향으로 가중치를 조금 바꾼 뒤 다시 손실함수를 계산해 봅시다. 위에서 계산한 값보다 조금 줄어드는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3521917591376414"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nu = 1e-2\n",
    "net.W -= nu * dW\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2층 신경망 클래스 구현\n",
    "기계학습은 위 과정을 반복하는 것입니다.\n",
    "1. 레이어의 수, 각 레이어마다 뉴런의 갯수, 가중치와 편향을 고려한 신경망 모델 제작\n",
    "2. 미니배치, 손실함수 설정\n",
    "3. 기울기벡터 구하기\n",
    "4. 매개변수(가중치, 편향 값) 갱신\n",
    "5. 1-4 반복\n",
    "\n",
    "위 방법을 적용하여 2층 신경망 클래스를 구현하고 MNIST 데이터셋으로 검증해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_minibatch(y, t)\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient_2d(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient_1d(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient_2d(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient_1d(loss_W, self.params['b2'])\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6.895989201366559\n",
      "1\n",
      "6.905181921338063\n",
      "2\n",
      "6.903950468022495\n",
      "3\n",
      "6.899529428496685\n",
      "4\n",
      "6.90045152824363\n",
      "5\n",
      "6.8949546933713854\n",
      "6\n",
      "6.906336893849032\n",
      "7\n",
      "6.884283457198426\n",
      "8\n",
      "6.890265357365474\n",
      "9\n",
      "6.89995117527855\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 10\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = .1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    print(i)\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    print(loss)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(len(train_loss_list))\n",
    "y = train_loss_list\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
